\section{GPU Implementation} \label{gpu-implementation}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{images/pipeline.png}
  \caption{GPU and CPU pipeline of the implementation}
  \label{fig:pipeline}
\end{figure}

\autoref{fig:pipeline} displays the GPU and CPU pipeline of the implementation.
The first step loads the image, which happens on the CPU.  The conversion of
the image from RGB to YUV on the GPU is run in parallel with the loading of the
pencil textures on the CPU.  The next kernel extracts the gray-scale version of
the image from the RGB to YUV result.  The gray-scale version is used for the
gradient calculation, which is then applied to the convolution filter.  This
accomplishes the line drawing step.  In preparation for the tone drawing step,
the CPU is computing the target histogram.  The histogram calculation in the
tone drawing step receives the gray-scale version as the input image.  The
result is being passed to the histogram mapping, together with the computed
target histogram.  Another kernel receives the loaded pencil textures and
expands them to the image size.  The expanded textures and the histogram
mapping are being used for the texture calculation.  The last kernel in the
tone drawing, the texture rendering kernel, computes the result of the tone
drawing.  The last two kernels represent the merge.  One kernel combines the
results of the line drawing and the tone drawing step and generates the final
image.  The last kernel applies the coloring if requested and converts the YUV
back to RGB.  The CPU takes care of saving the image.

\subsection{Sketch Filter}
Calculating the grayscale image $I_g$ and Gradient $G$ on the GPU is no big
challenge. However the efficient implementation of the line convolution
calculation is more interesting.

To calculate the value $L(x)$, first all $G_i(x)$ have to
be computed (see \autoref{eq:Gi}) and the maximum from all line convolution
results is selected (see \autoref{eq:L}). The following paragraphs describe the
implementation of the Cuda-Kernel, which computes $L$ directly from $G$ given
the desired line length and strength.

\paragraph{Compute the line convolutions} 
To compute the convolution result $G_i(x)$ for pixel $x$ and line $i$ all values of $G$
along the line segment $\mathscr{L}_i$ are collected and averaged. The
convolution-kernel of the line segment $\mathscr{L}_i$ can be described as 
\begin{align*}
  k_i(p) = \begin{cases}
    \frac{1}{\text{line length} \cdot \text{line strength}} & p \text{ is part
    of } \mathscr{L}_i\\
    0 & \text{otherwise}
  \end{cases}
\end{align*}
Collecting the right pixels for the right line is done by iterating over the
pixels of a horizontal line with the desired length and strength. This line
starts at $x$. Then the coordinates are rotated to get the pixels for line $i$.
All line convolutions for one pixel is calculated by a single thread. The
results for a line is keep as maximum if it is bigger than all its predecessors. 

Finally the inversion, a gamma correction and clipping is used to create the
final value for $L(x)$:
\begin{align*}
  L(x) = \max(255 - \max(G_i)^{\gamma}, 50)
\end{align*}
The $\gamma$ parameter can be used to intensify or weaken the lines.

\paragraph{Shared Memory}
The same pixel values from $G$ have to be loaded from neighboring quite often.
Therefore it makes sens to use shared memory to speed up memory access.
As each pixel is calculated by a individual thread, and we can only use 1024
threads in one block we get blocks of size $32\times32$, as shown in
\autoref{fig:shared-memory}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.2\textwidth]{images/shared-memory.png}
  \caption{Block and shared memory dimensions}
  \label{fig:shared-memory}
\end{figure}

On our device the maximum size of shared memory per block is $48kb$, so we can
only store a $109\times109$ block of float values. Therefore we only allow line
lengths up to 45 pixels.

The data from $G$ is copied by assigning the thread number to the linear
indexes of the data. As there is more data than threads, one thread copies
multiple data elements. The pattern is shown in \autoref{fig:shared-copy}.
\autoref{fig:shared-copy}.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{images/shared-copy.png}
  \caption{Thread number assignment when copying the data from $G$ to shared memory.}
  \label{fig:shared-copy}
\end{figure}

The coordinate calculations for the line convolution computations are done such
that they generate the correct coordinates for the shared memory block. Due to
the coordinate rotations the access patterns are very chaotic, which might lead
to unavoidable bank conflicts.

Coordinates which lie outside the input image must not be included in the
copying process and later to calculate the convolution results.



\subsection{Histogram}
The histogram calculation uses the shared memory per block to compute a partial
histogram within each block.  Each block computes an additional accumulative
histogram with the use of parallel prefix sum.  The partial results are being
combined at the end of the kernel per thread, by accumulating the results in
global memory.  As we require a 256-bin histogram the shared memory per block
is only two times 256 Integers of size, one for the simple histogram and one
for the accumulative histogram.

\subsection{Histogram Matching}
The histogram matching is used to adjust the tone distribution of the 
grayscale image to match a specific target tone distribution by using
their corresponding histogram. In this context, the target tone
distribution is artificially created to be similar to those of pencil
sketches as explained above.

The cumulative target histogram is created on the CPU by simply applying
the the parametric model to the values 0 to 255 and summing it up with
the values below.
This step is done on the CPU as the target distribution only has to
be evaluated 256 times which is negligible effort and can be either be
precomputed or done while the GPU is busy with other steps of the pipeline.

Once this histogram is uploaded to the GPU it can be used together with
the histogram from the image created in the previous step to adjust the
tones of the grayscale image. This is done in a kernel with one thread
per image pixel. A thread will the obtain the tone value of the pixel
it corresponds to and look up it's cumulative probability value in
the source histogram map.
Afterwards, the cumulative target histogram is used in inverse to find
out which tone the value corresponds to in it. The pixel is then set to
the looked up target tone value.

The inverse lookup in the cumulative target histogram is implemented
as a binary search function which is used in the kernel. Although
one would think that a binary search is inefficient when multiple threads
are executed in a warp and search for different values, it really isn't.
As the cumulative target histogram map only consist of 256 values,
the binary search will need at maximum $log_2 256=8$ loop runs to end
up at the correct value. Because the threads are executed in a warp,
the branches might get executed in serial. However, there is just one 
real branching operation inside the loop body, which could double the
effort. Still, the total effort negligible in contrast to a linear search
through the map with a worst case complexity of 256.

\subsection{Texturing}
The \autoref{eq:beta} has the same shape as a Tikhonov regularization and
therefore can be solved exactly the same way by solving the following normal
equation:
\begin{align}
  \underbrace{(A A^T + \lambda \cdot \Gamma \Gamma^T)}_{A^*} x = \underbrace{A^T
  b}_{b^*}
  \label{eq:tikhonov}
\end{align}
$x$ is the solution image $\beta$ in vector shape. $A$ is a diagonal matrix.
Each diagonal element corresponds to one pixel in $\ln(H)$. $b$ is the
vectorized $\ln(J)$ image and $\Gamma$ is a diagonal matrix with two diagonals
which represents if multiplied to a vectorized image the gradient operator.

The iterative Conjugate Gradient solver from the cuda sparce library
(\texttt{cusp}) is used to solve the linear system $A^* x = b^*$.

For this the input images $\ln(J)$ and $\ln(H)$ are downloaded from the GPU,
feed into diagonal matrices from \texttt{cusp} then again uploaded to solve the
equation, once more downloaded and uploaded for the final composition with the
line image $L$.

Matrix $A*$ and vector $b^*$ (\autoref{eq:tikhonov}) could be calculated using
\texttt{cusp}. For this one would upload $\ln(H)$, $\ln(H)$ and $\Gamma$, and
use the matrix operations included in \texttt{cusp} to calculate the
matrix-matrix and matrix-vector multiplications. However it is much more
effective to construct $A^*$ and $b^*$ on the CPU and upload just those results,
because \texttt{cusp} needs an output target matrix for each operation. So a lot
of unnecessary memory needs to be allocated and used for the calculations on the
GPU.  Due to the nice structure of the matrices, $A^*$ and $b^*$  can be
calculated in $\mathcal{O}(n)$, with $n =$ number of pixel in the input image.
This is illustrated in \autoref{fig:matrix-mult}.

\begin{figure}[htb]
  \centering
  {\small
  \begin{align*}
    A*= \begin{pmatrix}
      a & 0 & -\lambda & 0 & 0 & 0 & 0 & \dots\\
      0 & b & 0 & -\lambda & 0 & 0 & 0 & \dots\\
      -\lambda & 0 & b & 0 & -\lambda & 0 & 0 & \dots\\
      0 & -\lambda & 0 & b & 0 & -\lambda & 0 & \dots\\
      0 & 0 & -\lambda & 0 & b & 0 & -\lambda & \ddots\\
      \vdots& &  &  &\ddots  \\
      0 & 0 & \dots & 0 & -\lambda & 0 & b & 0 & -\lambda\\
      0 & 0 & \dots & 0 & 0 &-\lambda & 0 & b & 0\\
      0 & 0 & \dots & 0 & 0 & 0 &-\lambda & 0 & a \\
    \end{pmatrix}
  \end{align*}
}
  \caption{Closed form for Matrix $A^*$. Substitute $a = \lambda +
  \ln(H(i))^2$ and substitute $b = 2\lambda + \ln(H(i))^2$ with $i$ being the line
in the matrix.}
  \label{fig:matrix-mult}
\end{figure}

According to our tests assembling the Matrix on the CPU gains a total speedup
of the program of factor 2.02 compared to the GPU method.
