\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Lang:2012}
\citation{Weickert:1998}
\citation{GastalOliveira2011DomainTransform}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A application of the presented method is to consistently propagate user input scribbles through a video. Optical flow (c) is calculated from the input video (a) using sparse feature matching. Then the scribbles (b) are spread in space and time to generate (d)}}{1}{figure.1}}
\newlabel{fig:scribble}{{1}{1}{A application of the presented method is to consistently propagate user input scribbles through a video. Optical flow (c) is calculated from the input video (a) using sparse feature matching. Then the scribbles (b) are spread in space and time to generate (d)}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{GastalOliveira2011DomainTransform}
\citation{conf/psivt/HosniRBG11}
\citation{conf/acivs/HoffkenOK11}
\citation{Levin:2004:CUO:1015706.1015780}
\citation{Weickert:1998}
\citation{paris2009bilateral}
\@writefile{toc}{\contentsline {section}{\numberline {2}Previous Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{2}{section.3}}
\newlabel{method}{{3}{2}{Method}{section.3}{}}
\newlabel{eq:error-function}{{1}{2}{Method}{equation.3.1}{}}
\newlabel{eq:optical-flow-data}{{2}{2}{Method}{equation.3.2}{}}
\newlabel{eq:optical-flow-smooth}{{3}{2}{Method}{equation.3.3}{}}
\newlabel{eq:heat-equation}{{4}{2}{Method}{equation.3.4}{}}
\newlabel{eq:initial-condition}{{5}{2}{Method}{equation.3.5}{}}
\citation{GastalOliveira2011DomainTransform}
\newlabel{domain-transform}{{3}{3}{Domain Transform}{section*.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Domain Transform}{3}{section*.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 1D domain transform example. (a) Input signal, (b) transform of the sample distances, (c) Transformed signal, (d) Result of filtering (c) with Gaussian filter and remap to original domain}}{3}{figure.2}}
\newlabel{fig:domain-transform-example}{{2}{3}{1D domain transform example. (a) Input signal, (b) transform of the sample distances, (c) Transformed signal, (d) Result of filtering (c) with Gaussian filter and remap to original domain}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Joint filtering. Left: Input image $I$ for domain transform; Center: Sparse color input $J$ generated by selecting 1000 random pixels from $I$, Right: Result of the joint filtering ($J''=\frac  {J'}{G'}$)}}{3}{figure.3}}
\newlabel{fig:joint-filter}{{3}{3}{Joint filtering. Left: Input image $I$ for domain transform; Center: Sparse color input $J$ generated by selecting 1000 random pixels from $I$, Right: Result of the joint filtering ($J''=\frac {J'}{G'}$)}{figure.3}{}}
\newlabel{temporal-filtering}{{3}{3}{Temporal Filtering}{section*.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Temporal Filtering}{3}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The motion path of one pixel. It is calculated by following the optical flow vectors (blue arrows). The temporal filter pass is applied to the resulting 1D color vector (a,b,c,d,e)}}{4}{figure.4}}
\newlabel{fig:motion-path}{{4}{4}{The motion path of one pixel. It is calculated by following the optical flow vectors (blue arrows). The temporal filter pass is applied to the resulting 1D color vector (a,b,c,d,e)}{figure.4}{}}
\newlabel{confidence}{{3}{4}{Confidence}{section*.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Confidence}{4}{section*.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Demonstration of the confidence values applied to a 1D signal. In the top row all samples have the same confidence. In the bottom row the confidence of the middle sample is reduced. The purple line shows the contribution of a point.}}{4}{figure.5}}
\newlabel{fig:confidence}{{5}{4}{Demonstration of the confidence values applied to a 1D signal. In the top row all samples have the same confidence. In the bottom row the confidence of the middle sample is reduced. The purple line shows the contribution of a point}{figure.5}{}}
\newlabel{iterative-occlusion-estimates}{{3}{4}{Iterative Occlusion Estimates}{section*.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Iterative Occlusion Estimates}{4}{section*.5}}
\newlabel{eq:penalty}{{6}{4}{Iterative Occlusion Estimates}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Evaluation}{4}{subsection.3.1}}
\newlabel{evaluation}{{3.1}{4}{Evaluation}{subsection.3.1}{}}
\citation{Rhemann:2011:FCF:2191740.2191908}
\citation{Zimmer:2011:OFH:1969654.1969711}
\citation{conf/iccv/VolzBVZ11}
\citation{journals/tog/LiuTFDA05}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Shape of the penalty function for $\theta = 5$.}}{5}{figure.6}}
\newlabel{fig:penalty-shape}{{6}{5}{Shape of the penalty function for $\theta = 5$}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The effect of each processing step: temporal filtering, confidence and iterative occlusion estimates. The first two rows show color coded optical flow vector estimations, the last row is a scribble propagation which dyes a frog to be red.}}{5}{figure.7}}
\newlabel{fig:improvements}{{7}{5}{The effect of each processing step: temporal filtering, confidence and iterative occlusion estimates. The first two rows show color coded optical flow vector estimations, the last row is a scribble propagation which dyes a frog to be red}{figure.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison for the optical flow calculation to different approaches on the same 8 frame sequence ($640\times 640$ resolution)}}{5}{table.1}}
\newlabel{tab:performance}{{1}{5}{Performance comparison for the optical flow calculation to different approaches on the same 8 frame sequence ($640\times 640$ resolution)}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Applications}{5}{section.4}}
\newlabel{applications}{{4}{5}{Applications}{section.4}{}}
\newlabel{disparity-estimation}{{4}{5}{Disparity Estimation}{section*.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Disparity Estimation}{5}{section*.6}}
\newlabel{colorization}{{4}{5}{Colorization and Scribble Propagation}{section*.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Colorization and Scribble Propagation}{5}{section*.7}}
\bibstyle{alpha}
\bibdata{bibliography}
\bibcite{GastalOliveira2011DomainTransform}{GO11}
\bibcite{conf/acivs/HoffkenOK11}{HOK11}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The results of the disparity estimation is compared of a state of the art dataset provided by the MPEG group for testing. Again the results of the proposed method is more stable in time. Less holes are randomly appearing and disappearing compared to MPEG data set.}}{6}{figure.8}}
\newlabel{fig:disparity}{{8}{6}{The results of the disparity estimation is compared of a state of the art dataset provided by the MPEG group for testing. Again the results of the proposed method is more stable in time. Less holes are randomly appearing and disappearing compared to MPEG data set}{figure.8}{}}
\newlabel{up-sampling}{{4}{6}{Depth up-sampling}{section*.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Depth up-sampling}{6}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Low resolution and extremely noisy depth maps from a Microsoft Kinect is up-sampled and filled (white holes) using the same joint filtering operation as it is used for the optical flow calculation. The image data from the Kinect camera is used for the domain transform.}}{6}{figure.9}}
\newlabel{fig:depth-upsampling}{{9}{6}{Low resolution and extremely noisy depth maps from a Microsoft Kinect is up-sampled and filled (white holes) using the same joint filtering operation as it is used for the optical flow calculation. The image data from the Kinect camera is used for the domain transform}{figure.9}{}}
\newlabel{saliency}{{4}{6}{Saliency}{section*.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Saliency}{6}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces First three images show the per frame saliency calculation, which are then temporally filtered to create the smooth results on the right.}}{6}{figure.10}}
\newlabel{fig:saliency}{{10}{6}{First three images show the per frame saliency calculation, which are then temporally filtered to create the smooth results on the right}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}{section.5}}
\bibcite{conf/psivt/HosniRBG11}{HRBG11}
\bibcite{Levin:2004:CUO:1015706.1015780}{LLW04}
\bibcite{journals/tog/LiuTFDA05}{LTF{$^{+}$}05}
\bibcite{Lang:2012}{LWA{$^{+}$}12}
\bibcite{paris2009bilateral}{PKT09}
\bibcite{Rhemann:2011:FCF:2191740.2191908}{RHB{$^{+}$}11}
\bibcite{conf/iccv/VolzBVZ11}{VBVZ11}
\bibcite{Weickert:1998}{Wei98}
\bibcite{Zimmer:2011:OFH:1969654.1969711}{ZBW11}
